{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# John's Story â€” Learning with KNN ðŸ“˜\n",
    "\n",
    "John has just moved to a new, high-performing school.  \n",
    "Heâ€™s worried about whether heâ€™ll pass his exams.  \n",
    "\n",
    "To understand his chances, John looks at other students with similar study and sleep habits.  \n",
    "Weâ€™ll use the **k-Nearest Neighbors (KNN)** algorithm to see how he compares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## The Classmates\n",
    "\n",
    "We have data on four students:\n",
    "\n",
    "- **Alice**: studied 2 hours, slept 9 â†’ Fail (55%)\n",
    "- **Ben**: studied 4 hours, slept 8 â†’ Pass (70%)\n",
    "- **Cara**: studied 6 hours, slept 5 â†’ Pass (80%)\n",
    "- **Dan**: studied 8 hours, slept 3 â†’ Fail (90%)\n",
    "\n",
    "John: studied 5 hours, slept 7 â†’ **???**\n",
    "\n",
    "Our goal: predict Johnâ€™s outcome using KNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: [hours_studied, hours_slept]\n",
    "X = [\n",
    "    [2, 9],  # Alice\n",
    "    [4, 8],  # Ben\n",
    "    [6, 5],  # Cara\n",
    "    [8, 3],  # Dan\n",
    "]\n",
    "\n",
    "students = [\"Alice\", \"Ben\", \"Cara\", \"Dan\"]\n",
    "\n",
    "# Classification labels (Pass=1, Fail=0)\n",
    "y_class = [0, 1, 1, 0]  # (toy labels)\n",
    "\n",
    "# Regression labels (grades %)\n",
    "y_reg = [55, 70, 80, 90]\n",
    "\n",
    "# John (new student)\n",
    "john = [5, 7]\n",
    "\n",
    "X, y_class, y_reg, john"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1: Distances\n",
    "\n",
    "John learns that to compare himself to classmates,  \n",
    "he needs to calculate the **Euclidean distance** â€” the straight-line distance between his habits and theirs.\n",
    "\n",
    "This is where the vector math he studied earlier pays off:\n",
    "- Subtract coordinates â†’ get a difference vector\n",
    "- Find its magnitude â†’ the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "for name, p in zip(students, X):\n",
    "    dx = john[0] - p[0]\n",
    "    dy = john[1] - p[1]\n",
    "    d  = math.sqrt(dx*dx + dy*dy)\n",
    "    print(f\"John{john} -> {name}{p}: dx={dx}, dy={dy}, distance={d:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 2: Pass or Fail?\n",
    "\n",
    "John uses **k=3 neighbors**.  \n",
    "He looks at his 3 closest classmates:\n",
    "- Ben â†’ Pass\n",
    "- Cara â†’ Pass\n",
    "- Alice â†’ Fail\n",
    "\n",
    "2 out of 3 are Pass â†’ John is predicted to **Pass** âœ…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def euclidean(a, b):\n",
    "    \"\"\"Euclidean distance between two feature vectors a and b.\"\"\"\n",
    "    return math.sqrt(sum((ai - bi)**2 for ai, bi in zip(a, b)))\n",
    "\n",
    "def k_nearest_neighbors(X, x_new, k=3):\n",
    "    \"\"\"Return list of (distance, index) for the k closest points in X to x_new.\"\"\"\n",
    "    dists = [(euclidean(x, x_new), i) for i, x in enumerate(X)]\n",
    "    dists.sort(key=lambda t: t[0])\n",
    "    return dists[:k]\n",
    "\n",
    "def knn_classify(X, y_class, x_new, k=3):\n",
    "    \"\"\"Majority vote among k nearest neighbors. Tie-break by the single closest neighbor.\"\"\"\n",
    "    neigh = k_nearest_neighbors(X, x_new, k)\n",
    "    labels = [y_class[i] for (_, i) in neigh]\n",
    "    counts = Counter(labels).most_common()\n",
    "    # tie â†’ pick label of the closest neighbor\n",
    "    if len(counts) > 1 and counts[0][1] == counts[1][1]:\n",
    "        return y_class[neigh[0][1]]\n",
    "    return counts[0][0]\n",
    "\n",
    "def knn_regress_mean(X, y_reg, x_new, k=3):\n",
    "    \"\"\"Unweighted mean of neighbor targets.\"\"\"\n",
    "    neigh = k_nearest_neighbors(X, x_new, k)\n",
    "    vals = [y_reg[i] for (_, i) in neigh]\n",
    "    return sum(vals) / len(vals)\n",
    "\n",
    "def knn_regress_weighted(X, y_reg, x_new, k=3):\n",
    "    \"\"\"Distance-weighted average with weights = 1/d. Exact match returns that label.\"\"\"\n",
    "    neigh = k_nearest_neighbors(X, x_new, k)\n",
    "    num, den = 0.0, 0.0\n",
    "    for (d, i) in neigh:\n",
    "        if d == 0:\n",
    "            return y_reg[i]\n",
    "        w = 1.0 / d\n",
    "        num += w * y_reg[i]\n",
    "        den += w\n",
    "    return num / den\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "# Find k nearest neighbors to John\n",
    "neigh = k_nearest_neighbors(X, john, k=k)\n",
    "\n",
    "# Pretty print neighbor details\n",
    "print(\"k =\", k)\n",
    "print(\"Neighbors (name, features, class, distance):\")\n",
    "for (d, i) in neigh:\n",
    "    name = students[i]\n",
    "    feats = X[i]\n",
    "    label = y_class[i]  # 1=Pass, 0=Fail\n",
    "    print(f\"  {name:>5}  {feats}  class={label}  dist={d:.4f}\")\n",
    "\n",
    "# Predict class by majority vote\n",
    "pred_class = knn_classify(X, y_class, john, k=k)\n",
    "print(\"\\nPrediction (Pass=1, Fail=0):\", pred_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 3: What Grade Might John Get?\n",
    "\n",
    "Instead of just Pass/Fail, we use their percentages:\n",
    "\n",
    "- Ben: 70\n",
    "- Cara: 80\n",
    "- Alice: 55\n",
    "\n",
    "**Simple average:** (70+80+55)/3 = ~68.3%  \n",
    "**Weighted by distance:** closer classmates count more â†’ ~70.2%\n",
    "\n",
    "John feels reassured: heâ€™s likely to pass comfortably, with room to improve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict John's grade with KNN regression\n",
    "pred_reg_mean = knn_regress_mean(X, y_reg, john, k=3)\n",
    "pred_reg_weighted = knn_regress_weighted(X, y_reg, john, k=3)\n",
    "\n",
    "# Show neighbor details for regression\n",
    "neigh_reg = k_nearest_neighbors(X, john, k=3)\n",
    "print(\"Neighbors (name, grade, distance):\")\n",
    "for (d, i) in neigh_reg:\n",
    "    print(f\"  {students[i]:>5}  grade={y_reg[i]}  dist={d:.4f}\")\n",
    "\n",
    "print(f\"\\nUnweighted mean prediction: {pred_reg_mean:.2f}%\")\n",
    "print(f\"Distance-weighted prediction: {pred_reg_weighted:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 4: Beyond Study and Sleep\n",
    "\n",
    "John notices classmates who exercise, manage stress, and eat well often do even better.  \n",
    "So he extends his dataset with new features:\n",
    "- Hours of exercise per week\n",
    "- Stress score (0â€“10)\n",
    "- Nutrition quality (0â€“10)\n",
    "\n",
    "Now each student is a point in higher dimensions.  \n",
    "KNN still works the same way â€” it just compares across more features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended features: [study, sleep, exercise_hours, stress_0to10, nutrition_0to10]\n",
    "# (Toy values â€” tweak as you like)\n",
    "X_ext = [\n",
    "    [2, 9, 1, 7, 4],  # Alice: low exercise, high stress, modest nutrition\n",
    "    [4, 8, 3, 5, 6],  # Ben\n",
    "    [6, 5, 5, 4, 7],  # Cara\n",
    "    [8, 3, 4, 6, 5],  # Dan\n",
    "]\n",
    "\n",
    "# Keep the same grade labels for simplicity\n",
    "y_reg_ext = y_reg[:]  # [55, 70, 80, 90]\n",
    "\n",
    "# John's extended features (edit as you like)\n",
    "john_ext = [5, 7, 2, 5, 5]\n",
    "\n",
    "# Use k=3 neighbors\n",
    "k = 3\n",
    "\n",
    "# Show nearest neighbors in the extended space\n",
    "neigh_ext = k_nearest_neighbors(X_ext, john_ext, k=k)\n",
    "print(\"Extended neighbors (distance, name, features, grade):\")\n",
    "for (d, i) in neigh_ext:\n",
    "    print(f\"  dist={d:.4f}  {students[i]:>5}  {X_ext[i]}  grade={y_reg_ext[i]}\")\n",
    "\n",
    "# Predict grade with mean and distance-weighted averages in higher-D\n",
    "pred_ext_mean = knn_regress_mean(X_ext, y_reg_ext, john_ext, k=k)\n",
    "pred_ext_weighted = knn_regress_weighted(X_ext, y_reg_ext, john_ext, k=k)\n",
    "\n",
    "print(f\"\\nExtended regression (mean):     {pred_ext_mean:.2f}%\")\n",
    "print(f\"Extended regression (weighted): {pred_ext_weighted:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 5: Scaling Matters\n",
    "\n",
    "John realizes that if one feature is on a much bigger scale  \n",
    "(e.g., exam prep time in minutes vs. stress score 0â€“10),  \n",
    "it can overwhelm the distances.\n",
    "\n",
    "Thatâ€™s why we use **normalization**: to keep features comparable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale_columns(M):\n",
    "    \"\"\"Column-wise min-max scaling to [0,1].\"\"\"\n",
    "    cols = list(zip(*M))\n",
    "    scaled_cols = []\n",
    "    for col in cols:\n",
    "        cmin, cmax = min(col), max(col)\n",
    "        if cmax == cmin:\n",
    "            scaled_cols.append([0.0]*len(col))\n",
    "        else:\n",
    "            scaled_cols.append([(v - cmin)/(cmax - cmin) for v in col])\n",
    "    return [list(row) for row in zip(*scaled_cols)]\n",
    "\n",
    "# Example: exaggerate one feature (add exam prep minutes, huge numbers)\n",
    "X_ext_unscaled = [row[:] for row in X_ext]\n",
    "john_ext_unscaled = john_ext[:]\n",
    "\n",
    "# Add a big feature (toy values)\n",
    "for row in X_ext_unscaled:\n",
    "    row.append(5000)  # all students have big values, just for illustration\n",
    "john_ext_unscaled = john_ext_unscaled + [4500]\n",
    "\n",
    "# Neighbors BEFORE scaling\n",
    "neigh_before = k_nearest_neighbors(X_ext_unscaled, john_ext_unscaled, k=3)\n",
    "\n",
    "# Apply min-max scaling\n",
    "X_scaled = minmax_scale_columns(X_ext_unscaled + [john_ext_unscaled])\n",
    "X_ext_scaled, john_ext_scaled = X_scaled[:-1], X_scaled[-1]\n",
    "\n",
    "# Neighbors AFTER scaling\n",
    "neigh_after = k_nearest_neighbors(X_ext_scaled, john_ext_scaled, k=3)\n",
    "\n",
    "print(\"Neighbors BEFORE scaling (distance, index):\", [(round(d,4), i) for (d,i) in neigh_before])\n",
    "print(\"Neighbors AFTER  scaling (distance, index):\", [(round(d,4), i) for (d,i) in neigh_after])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through KNN, John saw:\n",
    "- How his study/sleep compared to classmates\n",
    "- That heâ€™s on track to pass (~70%)\n",
    "- That adding lifestyle factors could refine predictions\n",
    "\n",
    "Most importantly, he discovered that math heâ€™d learned earlier â€” vectors, magnitudes, distances â€”  \n",
    "was the key to making sense of it all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "# Convert to NumPy\n",
    "X_np   = np.array(X)        # [[2,9],[4,8],[6,5],[8,3]]\n",
    "yC_np  = np.array(y_class)  # [0,1,1,0]\n",
    "yR_np  = np.array(y_reg)    # [55,70,80,90]\n",
    "john_np = np.array(john).reshape(1, -1)  # [[5,7]]\n",
    "\n",
    "k = 3\n",
    "\n",
    "# --- Classification ---\n",
    "clf = KNeighborsClassifier(n_neighbors=k)\n",
    "clf.fit(X_np, yC_np)\n",
    "pred_class = clf.predict(john_np)[0]\n",
    "proba = clf.predict_proba(john_np)[0]  # [P(Fail), P(Pass)]\n",
    "dists, idxs = clf.kneighbors(john_np, n_neighbors=k, return_distance=True)\n",
    "\n",
    "print(\"=== KNN Classification (2D) ===\")\n",
    "print(\"Prediction (Pass=1, Fail=0):\", pred_class)\n",
    "print(\"Class probabilities [Fail, Pass]:\", proba)\n",
    "print(\"Neighbors (distance, name, features, class):\")\n",
    "for d, i in zip(dists[0], idxs[0]):\n",
    "    print(f\"  {d:.4f}  {students[i]:>5}  {X[i]}  class={y_class[i]}\")\n",
    "\n",
    "# --- Regression ---\n",
    "reg = KNeighborsRegressor(n_neighbors=k)\n",
    "reg.fit(X_np, yR_np)\n",
    "pred_grade = reg.predict(john_np)[0]\n",
    "dists_r, idxs_r = reg.kneighbors(john_np, n_neighbors=k, return_distance=True)\n",
    "\n",
    "print(\"\\n=== KNN Regression (2D) ===\")\n",
    "print(f\"Predicted grade (%): {pred_grade:.2f}\")\n",
    "print(\"Neighbors (distance, name, features, grade):\")\n",
    "for d, i in zip(dists_r[0], idxs_r[0]):\n",
    "    print(f\"  {d:.4f}  {students[i]:>5}  {X[i]}  grade={y_reg[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use your extended feature matrix X_ext and john_ext from earlier\n",
    "X_ext_np   = np.array(X_ext)\n",
    "yR_ext_np  = np.array(y_reg)  # same grades for simplicity\n",
    "john_ext_np = np.array(john_ext).reshape(1, -1)\n",
    "\n",
    "k = 3\n",
    "\n",
    "# Pipeline: Standardize features -> KNN\n",
    "reg_ext = Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsRegressor(n_neighbors=k))\n",
    "])\n",
    "reg_ext.fit(X_ext_np, yR_ext_np)\n",
    "\n",
    "pred_ext = reg_ext.predict(john_ext_np)[0]\n",
    "\n",
    "# To see neighbors/distances after scaling, access the fitted KNN and scaled arrays\n",
    "X_ext_scaled = reg_ext.named_steps[\"scale\"].transform(X_ext_np)\n",
    "john_ext_scaled = reg_ext.named_steps[\"scale\"].transform(john_ext_np)\n",
    "knn_model = reg_ext.named_steps[\"knn\"]\n",
    "dists_ext, idxs_ext = knn_model.kneighbors(john_ext_scaled, n_neighbors=k, return_distance=True)\n",
    "\n",
    "print(\"=== KNN Regression (Extended + Scaled) ===\")\n",
    "print(f\"Predicted grade (%): {pred_ext:.2f}\")\n",
    "print(\"Neighbors in scaled space (distance, name, features, grade):\")\n",
    "for d, i in zip(dists_ext[0], idxs_ext[0]):\n",
    "    print(f\"  {d:.4f}  {students[i]:>5}  {X_ext[i]}  grade={y_reg[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
