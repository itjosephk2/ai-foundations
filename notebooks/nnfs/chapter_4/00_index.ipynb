{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee30363",
   "metadata": {},
   "source": [
    "# NNFS — Activation Functions (Chapter Index)\n",
    "\n",
    "> Chapter: Activation Functions  \n",
    "> Approx. pages: 72–108\n",
    "\n",
    "| #  | Book section                            | Pages | Suggested notebook                                      | Suggested script                               | What you’d do here |\n",
    "|----|-----------------------------------------|-------|--------------------------------------------------------|-----------------------------------------------|--------------------|\n",
    "| 01 | Activation Functions                    | 72    | `notebooks/nnfs/chapter_3/01_activation_functions.ipynb` | `src/nnfs/chapter_3/01_activation_functions.py` | High-level overview: what activation functions are and why we apply them after the dense layer. |\n",
    "| 02 | The Step Activation Function            | 73    | `notebooks/nnfs/chapter_3/02_step_activation.ipynb`       | `src/nnfs/chapter_3/02_step_activation.py`       | Implement step function, play with thresholds, show why it’s too “chunky” for most modern nets. |\n",
    "| 03 | The Linear Activation Function          | 74    | `notebooks/nnfs/chapter_3/03_linear_activation.ipynb`     | `src/nnfs/chapter_3/03_linear_activation.py`     | Identity / linear activation, show that stacking pure linear layers is just one big linear map. |\n",
    "| 04 | The Sigmoid Activation Function         | 75    | `notebooks/nnfs/chapter_3/04_sigmoid_activation.ipynb`    | `src/nnfs/chapter_3/04_sigmoid_activation.py`    | Implement σ(x), plot the S-shape, talk about squashing to (0,1) and gradient issues at extremes. |\n",
    "| 05 | The Rectified Linear Activation Function| 76    | `notebooks/nnfs/chapter_3/05_relu_activation.ipynb`       | `src/nnfs/chapter_3/05_relu_activation.py`       | Basic ReLU: max(0,x), visualize, compare to sigmoid / linear and discuss dead neurons briefly. |\n",
    "| 06 | Why Use Activation Functions?           | 77    | `notebooks/nnfs/chapter_3/06_why_activation_functions.ipynb` | `src/nnfs/chapter_3/06_why_activation_functions.py` | Show linear vs nonlinear decision boundaries, demonstrate what happens without nonlinearity. |\n",
    "| 07 | Linear Activation in the Hidden Layers  | 79    | `notebooks/nnfs/chapter_3/07_linear_in_hidden_layers.ipynb` | `src/nnfs/chapter_3/07_linear_in_hidden_layers.py` | Concrete example: multiple linear layers collapsing into one, with a small numeric demo. |\n",
    "| 08 | ReLU Activation in a Pair of Neurons    | 81    | `notebooks/nnfs/chapter_3/08_relu_pair_of_neurons.ipynb`   | `src/nnfs/chapter_3/08_relu_pair_of_neurons.py`   | Visualize how two ReLU neurons can carve up input space; piecewise linear regions intuition. |\n",
    "| 09 | ReLU Activation in the Hidden Layers    | 85    | `notebooks/nnfs/chapter_3/09_relu_in_hidden_layers.ipynb`  | `src/nnfs/chapter_3/09_relu_in_hidden_layers.py`  | Extend ReLU intuition to multiple hidden neurons/layers; show richer nonlinear boundaries. |\n",
    "| 10 | ReLU Activation Function Code           | 95    | `notebooks/nnfs/chapter_3/10_relu_activation_code.ipynb`   | `src/nnfs/chapter_3/10_relu_activation_code.py`   | Proper class for ReLU (like you did for dense layer), forward pass on batches with NumPy. |\n",
    "| 11 | The Softmax Activation Function         | 98    | `notebooks/nnfs/chapter_3/11_softmax_activation.ipynb`     | `src/nnfs/chapter_3/11_softmax_activation.py`     | Implement Softmax, stabilize it (subtract max), interpret outputs as probabilities. |\n",
    "| 12 | Full code up to this point              | 108   | `notebooks/nnfs/chapter_3/12_full_code_chapter_3.ipynb`    | `src/nnfs/chapter_3/12_full_code_chapter_3.py`    | Collate dense layer + activations into a tiny “mini-framework” and test on sample inputs. |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
