{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1a41e09",
   "metadata": {},
   "source": [
    "# NNFS – Chapter 3: Adding Layers\n",
    "\n",
    "In this notebook we follow along with **Neural Networks from Scratch** (NNFS) Chapter 3: *Adding Layers*.\n",
    "\n",
    "The goal is to:\n",
    "- Start with a batch of inputs (each with 4 features).\n",
    "- Pass them through a first dense layer with 3 neurons.\n",
    "- Use the outputs of that layer as inputs to a **second** dense layer with 3 neurons.\n",
    "\n",
    "This is our first taste of a **multi-layer (deep) network**: inputs → hidden layer 1 → hidden layer 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b6930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)  # nicer printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13405d",
   "metadata": {},
   "source": [
    "## 1. Inputs\n",
    "\n",
    "We start with a **batch** of 3 samples. Each sample has 4 features.\n",
    "\n",
    "- Shape: `(3, 4)` → 3 rows (samples), 4 columns (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f15bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[ 1.   2.   3.   2.5]\n",
      " [ 2.   5.  -1.   2. ]\n",
      " [-1.5  2.7  3.3 -0.8]]\n",
      "Shape of inputs: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    [1.0, 2.0, 3.0, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8],\n",
    "]\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Shape of inputs:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880765b8",
   "metadata": {},
   "source": [
    "## 2. First hidden layer\n",
    "\n",
    "Our first dense layer has **3 neurons**. Each neuron has:\n",
    "\n",
    "- A weight for each input feature (so 4 weights per neuron).\n",
    "- A single bias value.\n",
    "\n",
    "So the weight matrix has shape `(3, 4)`:\n",
    "- 3 rows → 3 neurons\n",
    "- 4 columns → 4 inputs per neuron\n",
    "\n",
    "The biases are stored as a 1D array of length 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e3eda11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (layer 1):\n",
      " [[ 0.2   0.8  -0.5   1.  ]\n",
      " [ 0.5  -0.91  0.26 -0.5 ]\n",
      " [-0.26 -0.27  0.17  0.87]]\n",
      "Shape of weights (layer 1): (3, 4)\n",
      "Biases (layer 1): [2.  3.  0.5]\n"
     ]
    }
   ],
   "source": [
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87],\n",
    "]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "weights = np.array(weights)\n",
    "biases = np.array(biases)\n",
    "\n",
    "print(\"Weights (layer 1):\\n\", weights)\n",
    "print(\"Shape of weights (layer 1):\", weights.shape)\n",
    "print(\"Biases (layer 1):\", biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5810edca",
   "metadata": {},
   "source": [
    "### Forward pass through layer 1\n",
    "\n",
    "To get the outputs of the first layer, we use a **matrix product**:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{layer1\\_outputs} = X W^T + b\n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "- `X` is the input matrix with shape `(3, 4)`\n",
    "- `W` is the weight matrix with shape `(3, 4)`\n",
    "- `W.T` (the transpose) has shape `(4, 3)`\n",
    "- `b` is the bias vector with shape `(3,)`\n",
    "\n",
    "The result `layer1_outputs` has shape `(3, 3)` → one output per neuron for each of the 3 input samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ab597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 outputs:\n",
      " [[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n",
      "Shape of layer1_outputs: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "layer1_outputs = np.dot(inputs, weights.T) + biases\n",
    "print(\"Layer 1 outputs:\\n\", layer1_outputs)\n",
    "print(\"Shape of layer1_outputs:\", layer1_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe33f4",
   "metadata": {},
   "source": [
    "## 3. Second hidden layer\n",
    "\n",
    "Now we **add another layer**. This is what makes the network *deep*.\n",
    "\n",
    "Key rule:\n",
    "- The number of inputs to a layer must match the number of outputs from the previous layer.\n",
    "\n",
    "Our first hidden layer has 3 neurons → it outputs 3 values per sample.\n",
    "So, each neuron in the second layer must have **3 weights** (one per output from layer 1).\n",
    "\n",
    "We again choose 3 neurons for the second layer, so:\n",
    "- Weight matrix: shape `(3, 3)`\n",
    "- Bias vector: shape `(3,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed398c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (layer 2):\n",
      " [[ 0.1  -0.14  0.5 ]\n",
      " [-0.5   0.12 -0.33]\n",
      " [-0.44  0.73 -0.13]]\n",
      "Shape of weights (layer 2): (3, 3)\n",
      "Biases (layer 2): [-1.   2.  -0.5]\n"
     ]
    }
   ],
   "source": [
    "weights2 = [\n",
    "    [0.1, -0.14, 0.5],\n",
    "    [-0.5, 0.12, -0.33],\n",
    "    [-0.44, 0.73, -0.13],\n",
    "]\n",
    "biases2 = [-1.0, 2.0, -0.5]\n",
    "\n",
    "weights2 = np.array(weights2)\n",
    "biases2 = np.array(biases2)\n",
    "\n",
    "print(\"Weights (layer 2):\\n\", weights2)\n",
    "print(\"Shape of weights (layer 2):\", weights2.shape)\n",
    "print(\"Biases (layer 2):\", biases2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd9b50",
   "metadata": {},
   "source": [
    "### Forward pass through layer 2\n",
    "\n",
    "Now the **inputs to layer 2** are the outputs from layer 1 (`layer1_outputs`).\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{layer2\\_outputs} = \\text{layer1\\_outputs} \\cdot W_2^T + b_2\n",
    "\\end{align}\n",
    "\n",
    "Shapes:\n",
    "- `layer1_outputs`: `(3, 3)`\n",
    "- `weights2.T`: `(3, 3)`\n",
    "- result: `(3, 3)` (3 samples × 3 neurons in layer 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ef0077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 outputs:\n",
      " [[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n",
      "Shape of layer2_outputs: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "layer2_outputs = np.dot(layer1_outputs, weights2.T) + biases2\n",
    "print(\"Layer 2 outputs:\\n\", layer2_outputs)\n",
    "print(\"Shape of layer2_outputs:\", layer2_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f1279",
   "metadata": {},
   "source": [
    "If you're following along with the book, your values should match (up to minor rounding):\n",
    "\n",
    "```text\n",
    "array([[ 0.5031 , -1.04185, -2.03875],\n",
    "       [ 0.2434 , -2.7332 , -5.7633 ],\n",
    "       [-0.99314,  1.41254, -0.35655]])\n",
    "```\n",
    "\n",
    "Small differences usually mean you've changed weights, biases, or print precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc80da",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- Built a batch of inputs with 4 features each.\n",
    "- Defined a first dense layer (3 neurons) and computed its outputs.\n",
    "- Used those outputs as inputs to a second dense layer (3 neurons).\n",
    "- Saw how the **shapes** of inputs, weights, and outputs must line up.\n",
    "\n",
    "Conceptually, we now have a small **2-layer neural network** (both layers hidden for now). In later chapters, we'll add activation functions, an output layer, and eventually train the network using real data and gradient descent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
